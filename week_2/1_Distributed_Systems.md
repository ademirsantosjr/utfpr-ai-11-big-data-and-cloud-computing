
# Week 2: Big Data, Cloud Computing and the World of Distributed Systems

In this second week of studies in Big Data and Cloud Computing, the focus is on **Distributed Systems** and **Apache Hadoop**. The content has been organized into three main sections:

1. **Distributed Systems**: An introduction to the fundamental concepts.
2. **Apache Hadoop**: An overview of the tool.
3. **HDFS (Hadoop Distributed File System)**: A deep dive into Hadoop's data storage system.

This document is dedicated to the first section, exploring the universe of distributed systems.

## The Big Data Era: Applications and Challenges

The exponential growth in the volume of data generated by modern applications has driven the need for new computing architectures. Let's look at two emblematic examples:

### Autonomous Vehicles

Considered "mini data centers on wheels," autonomous vehicles generate a massive amount of data from cameras and multiple sensors. The challenge lies in the **capture, processing, and analysis** of this data in real time to ensure safe and efficient driving. The speed and volume of data are the main obstacles to be overcome.

> According to Bosch, one of the biggest challenges of autonomous driving is managing and analyzing the enormous amount of data generated by the vehicle to validate the systems' results.

### Genetic Sequencing

The field of genomics also faces the challenge of processing large volumes of data. The analysis of genomes for the development of new drugs requires immense computational power.

> AstraZeneca, for example, reported that the analysis of 88 human genomes consumed **15,000 hours** and generated **171 terabytes** of data.

These examples, along with use cases in retail, telecommunications, and the banking sector, demonstrate that although the technology to process large volumes of data exists, the search for higher performance is still an active field of research.

## What are Distributed Systems?

To deal with the challenges of Big Data, distributed systems present themselves as a robust solution. The literature offers us some definitions:

*   *"A collection of computers that appear to users as a single system."*
    *   **Key point**: **Transparency** is fundamental. The user does not need to know that they are interacting with a complex network of machines.
*   *"Systems in which hardware or software components are located on networked computers and communicate and coordinate their actions by exchanging messages."*
    *   **Key point**: The architecture is based on communication between distinct components via the network.
*   *"Systems that distribute the load among several devices on which the calculations are performed."*
    *   **Key point**: Load distribution is the essence of achieving the necessary performance, analogous to dividing a heavy load among several people.

Common examples of distributed systems include the **Internet**, **intranets**, and **Cloud Computing**.

### Essential Characteristics

Distributed systems have striking characteristics that, while making them challenging, offer the tools for Big Data processing:

| Characteristic | Description |
| :--- | :--- |
| **Communication between Devices** | Uses defined protocols for the exchange of information. |
| **Synchronization** | Ensures that events, data, and messages are processed at the appropriate time. |
| **Transparency** | Hides the complexity of the distribution from the end user. |
| **Data Transfer** | Mechanisms for moving large volumes of data between network nodes. |
| **Fault Tolerance** | Maintains application availability even with the failure of tasks or components. |
| **Scalability** | Ability to adapt (increase or decrease) computational resources according to demand. |

## When to Opt for a Distributed System?

The decision to use a distributed architecture is generally motivated by two main needs:

1.  **Higher Performance**: When a single server can no longer meet the demand, either due to the volume of data or the need for processing.
    *   **Solutions**: Load balancing and replication of processing power.
2.  **Greater Reliability**: When the focus is not only on performance, but also on ensuring high availability and redundancy of the application.
    *   **Solutions**: Data redundancy and planning for failure analysis.

Designing a system for high availability is not a simple task and requires careful planning of the architecture.

It is from these challenges and needs that tools like **Apache Hadoop** emerge, which will be the subject of our next study.
